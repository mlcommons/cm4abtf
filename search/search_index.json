{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About the POC","text":"<p>MLCommons and AVCC have jointly defined and developed the MLPerf: Inference Automotive POC which the first release and subset of the MLPerf: Inference Automotive benchmark suite. The benchmark suite has been made possbile with MLCommons expertise in AI benchmarks and AVCC's expertise in automotive. Note that as this is a POC, not the full benchmark suite, there will be no official MLCommons submissions. Find the POC reference block diagram below.</p> <p></p> <p>For ease of use, many of the benchmark components are packaged in a container which, e.g., reduces the software requirements on the computer running the benchmark. Please note that the POC reference is not for performance purposes; it is a reference to enable anyone to familiarize themselves with the benchmark and to give an understanding of how it works. For performant versions of the POC each HW/SW vendor will have their own optimized version of the model for their target hardware.</p> <p>To try out the POC reference yourself, please visit the Run section. The POC reference only has 20 frames, to see the object detection model run on an entire sequence, follow this link:</p> <p></p> <p>In the sections below the components of the benchmark are described.</p>"},{"location":"#collective-mind","title":"Collective Mind","text":"<p>Collective Mind (CM) is the automation and reproducibility framework/test harness. This framework is non-differentiating and sharing this amongst companies frees up time and resources which are better spent on optimizing models for the taget hardware. CM also makes collaboration easier as it's available open-source. Find more information about CM here:</p> <ul> <li>https://github.com/mlcommons/ck/tree/master/cm</li> </ul>"},{"location":"#ssd-resnet50","title":"SSD-ResNet50","text":"<p>SSD-ResNet50 v1.5 is a well-known object detection model which is one of the reasons it was selected the POC. It was agreed upon that the model is a representative model. Find more information about the models here:</p> <ul> <li>SSD: Single Shot MultiBox Detector</li> <li>Deep Residual Learning for Image Recognition</li> <li>SSD PyTorch</li> </ul>"},{"location":"#cognata-dataset","title":"Cognata Dataset","text":"<p>The SSD-ResNet50 model was trained on a synthetic dataset from Cognata. Only camera data from three frontfacing cameras was used for the POC and the resolution is 8 MP. Find the subset of the dataset for the POC here:</p> <ul> <li>Cognata Dataset for POC</li> </ul> <p>To gain access to the full dataset, an MLCommons membership is required, find more details here:</p> <ul> <li>MLCommons Cognata Dataset</li> </ul>"},{"location":"#additional-resources","title":"Additional Resources","text":""},{"location":"#avccs-technical-reports-trs","title":"AVCC's Technical Reports (TRs)","text":"<p>Find the TRs from AVCC on AI benchmarking recommendations for automotive here:</p> <ul> <li>TR-003 Conditions and Reporting</li> <li>TR-004 Models and Datasets</li> <li>TR-007 Compute Scenarios</li> </ul>"},{"location":"#join-the-mlperf-automotive-working-group","title":"Join the MLPerf Automotive Working Group","text":"<p>Take part of the joint MLCommons and AVCC working group to shape the full automotive ML benchmark suite:</p> <ul> <li>MLPerf Automotive Working Group</li> </ul>"},{"location":"benchmarks/","title":"MLPerf Inference Benchmarks","text":"<p>Please visit the individual benchmark links to see the run commands using the unified CM interface.</p> <ol> <li>Object Detection using SSD ResNet50 model and Cognata dataset</li> </ol>"},{"location":"benchmarks/object_detection/retinanet/","title":"Object Detection using Retinanet","text":""},{"location":"benchmarks/object_detection/retinanet/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>Retinanet validation run uses the OpenImages v6 MLPerf validation dataset resized to 800x800 and consisting of 24,576 images.</p> <p>Retinanet calibration dataset consist of 500 images selected from the OpenImages v6 dataset.</p> <pre><code>cm run script --tags=get,dataset,openimages,_calibration -j\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,openimages,_validation -j\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Retinanet Model</p> PytorchOnnx"},{"location":"benchmarks/object_detection/retinanet/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,retinanet,_pytorch -j\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#onnx","title":"Onnx","text":"<pre><code>cm run script --tags=get,ml-model,retinanet,_onnx -j\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#benchmark-implementations","title":"Benchmark Implementations","text":"MLCommons-PythonNvidiaIntelQualcommMLCommon-C++"},{"location":"benchmarks/object_detection/retinanet/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_1","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_2","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_3","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_1","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_1","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_4","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_1","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_5","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_1","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_6","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_1","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_7","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device","title":"ROCm device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_2","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_2","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_8","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_2","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_9","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_2","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_10","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_2","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_11","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_1","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_3","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_3","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_12","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_3","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_13","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_3","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_14","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_3","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_15","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_1","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_4","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_4","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_16","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_4","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_17","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_4","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_18","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_4","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_19","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_1","title":"ROCm device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_5","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_5","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_20","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_5","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_21","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_5","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_22","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_5","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_23","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework_1","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_2","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_6","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_6","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_24","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_25","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_6","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_26","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_2","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_7","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_7","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_27","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_1","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_28","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_7","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_29","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_2","title":"ROCm device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_8","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_8","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_30","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_2","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_31","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_8","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_32","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_3","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_9","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_9","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_33","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_3","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_34","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_9","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_35","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_3","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_10","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_10","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_36","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_4","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_37","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_10","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_38","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_3","title":"ROCm device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_11","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_11","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_39","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_5","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_40","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_11","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_41","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_1","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/object_detection/retinanet/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_4","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_12","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_12","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_42","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_6","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_43","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_6","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_44","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_12","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_45","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/object_detection/retinanet/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_5","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_13","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_13","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_46","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_6","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_47","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_13","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_48","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_2","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_4","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_14","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_14","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_49","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_7","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_50","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_7","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_51","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_14","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_52","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_5","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_15","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_15","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_53","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_7","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_54","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_15","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_55","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_3","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/object_detection/retinanet/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/object_detection/retinanet/#qaic-device","title":"QAIC device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_16","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_16","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_56","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_8","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_57","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_8","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_58","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_16","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_59","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/object_detection/retinanet/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/object_detection/retinanet/#qaic-device_1","title":"QAIC device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_17","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_17","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_60","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_8","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_61","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_17","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_62","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#mlperf-modular-implementation-in-c","title":"MLPerf Modular Implementation in C++","text":"edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_4","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework_2","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_6","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_18","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_18","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_63","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_9","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_64","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_9","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_65","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_18","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_66","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_6","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_19","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_19","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_67","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_10","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_68","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_10","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_69","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_19","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_70","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework_3","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_7","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_20","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_20","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_71","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_9","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_72","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_20","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_73","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_7","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_21","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_21","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_74","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_10","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_75","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_21","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_76","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"changelog/","title":"What's New, What's Coming","text":""},{"location":"changelog/changelog/","title":"Release Notes","text":""},{"location":"install/","title":"Installation","text":"<p>We use MLCommons CM Automation framework to run MLPerf inference benchmarks.</p>"},{"location":"install/#cm-install","title":"CM Install","text":"<p>We have successfully tested CM on</p> <ul> <li>Ubuntu 18.x, 20.x, 22.x , 23.x, </li> <li>RedHat 8, RedHat 9, CentOS 8</li> <li>macOS</li> <li>Wndows 10, Windows 11</li> </ul> UbuntuRed HatmacOSWindows <p>Please visit the official CM installation page for more details</p>"},{"location":"install/#ubuntu-debian","title":"Ubuntu, Debian","text":"<pre><code>   sudo apt update &amp;&amp; sudo apt upgrade\n   sudo apt install python3 python3-pip python3-venv git wget curl\n</code></pre> <p>Note that you must set up virtual env on Ubuntu 23+ before using any Python project: <pre><code>   python3 -m venv cm\n   source cm/bin/activate\n</code></pre></p> <p>You can now install CM via PIP:</p> <pre><code>   python3 -m pip install cmind\n</code></pre> <p>You might need to do the following command to update the <code>PATH</code> to include the BIN paths from pip installs</p> <pre><code>   source $HOME/.profile\n</code></pre> <p>You can check that CM is available by checking the <code>cm</code> command</p>"},{"location":"install/#red-hat","title":"Red Hat","text":"<pre><code>   sudo dnf update\n   sudo dnf install python3 python-pip git wget curl\n   python3 -m pip install cmind --user\n</code></pre>"},{"location":"install/#macos","title":"macOS","text":"<p>Note that CM currently does not work with Python installed from the Apple Store.  Please install Python via brew as described below.</p> <p>If <code>brew</code> package manager is not installed, please install it as follows (see details here): <pre><code>   /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre></p> <p>Don't forget to add brew to PATH environment as described in the end of the installation output.</p> <p>Then install python, pip, git and wget:</p> <pre><code>   brew install python3 git wget curl\n   python3 -m pip install cmind\n</code></pre>"},{"location":"install/#windows","title":"Windows","text":"<ul> <li>Configure Windows 10+ to support long paths from command line as admin:    <pre><code>   reg add \"HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\" /v LongPathsEnabled /t REG_DWORD /d 1 /f\n</code></pre> </li> <li>Download and install Git from git-for-windows.github.io.</li> <li>Configure Git to accept long file names: <code>git config --system core.longpaths true</code></li> <li>Download and install Python 3+ from www.python.org/downloads/windows.</li> <li>Don't forget to select option to add Python binaries to PATH environment!</li> <li> <p>Configure Windows to accept long fie names during Python installation!</p> </li> <li> <p>Install CM via PIP:</p> </li> </ul> <pre><code>   python -m pip install cmind\n</code></pre> <p>Note that we have reports   that CM does not work when Python was first installed from the Microsoft Store.  If CM fails to run, you can find a fix here.</p>"},{"location":"install/#download-the-cm-mlops-repository","title":"Download the CM MLOps Repository","text":"<pre><code>   cm pull repo gateoverflow@cm4mlops\n</code></pre> <p>Now, you are ready to use the <code>cm</code> commands to run MLPerf inference as given in the benchmarks page</p>"},{"location":"run/","title":"Run the POC Reference","text":""},{"location":"run/#install-dependencies","title":"Install Dependencies","text":"<p>MLCommons CM Automation framework is used to run the POC reference. CM requires Python 3.7+, git, python3-pip and python3-venv. If these dependencies are present you can do</p> <p>Activate a venv for CM (Not mandatory but recommended) <pre><code>python3 -m venv cm\nsource cm/bin/activate\n</code></pre></p> <p>Install CM and pull the needed repositories <pre><code>pip install cm4abtf\n</code></pre></p> <p>More installation details can be found at CM Installation Page</p> <p>Using an Ubuntu example, run <code>cm</code> in the terminal and if CM successfully installed, expect the following output:</p> <pre><code>(cm) user@ubuntu:~$ cm \n cm {action} {automation} {artifact(s)} {--flags} @input.yaml @input.json\n</code></pre> <p>Note: the <code>(cm)</code> indicates that Python <code>venv</code> is active.</p> <p>Now, the <code>cm</code> cli commands will be used to run the POC reference.</p> <p>The computer running the POC reference needs Docker. Follow instructions described in the link below:</p> <ul> <li>Install Docker Engine</li> </ul> <p>Note: if you're running Ubuntu, CM automatically installs Docker when running the benchmark. We have tested the run on Ubuntu, RHEL, macOS and Windows</p>"},{"location":"run/#start-the-benchmark","title":"Start the Benchmark","text":"<p>By running the script below, you are downloading the POC container and dataset, then launching the benchmark. All in one command!</p> DockerNative <p>Depending on the computer used and internet connection, this can take a few minutes.</p>"},{"location":"run/#docker","title":"Docker","text":"<p>Set up docker <pre><code>cm run script --tags=run-abtf,_poc-demo --quiet \\\n --docker --docker_cache=no\n</code></pre></p> <p>Tip</p> <ul> <li>Use <code>--env.CM_MLPERF_LOADGEN_BUILD_FROM_SRC=off</code> to use the prebuilt MLPerf Loadgen binary and not do a source compilation</li> <li>Use <code>--docker_os=[rhel|arch|ubuntu]</code> to change the docker OS</li> <li>Use <code>--docker_os_version=[8|9]</code> for <code>RHEL</code>, <code>[24.04|22.04|20.04]</code> for <code>ubuntu</code> and <code>[latest]</code> for <code>arch</code> </li> <li>Use <code>--docker_base_image=[IMAGE_NAME]</code> to override the default base image for docker</li> <li>Github actions for this run can be seen here</li> </ul> <p>Run Command (inside docker) <pre><code>cm run script --tags=run-abtf,_poc-demo --quiet\n</code></pre></p> <p>Tip</p> <ul> <li>Use <code>--rerun</code> to force overwrite the previously generated results</li> <li>Use <code>--env.CM_MLPERF_LOADGEN_BUILD_FROM_SRC=off</code> to use the prebuilt MLPerf Loadgen binary and not do a source compilation</li> </ul> <p>We have tested the workflow via docker on Ubuntu 24.04, Ubuntu 22.04, Ubuntu 20.04, Windows 11 and macOS 14</p>"},{"location":"run/#native","title":"Native","text":"<pre><code>cm run script --tags=run-abtf,_poc-demo --quiet\n</code></pre> <p>Tip</p> <ul> <li>Use <code>--rerun</code> to force overwrite the previously generated results</li> <li>Use <code>--env.CM_MLPERF_LOADGEN_BUILD_FROM_SRC=off</code> to use the prebuilt MLPerf Loadgen binary and not do a source compilation</li> <li>Use <code>--adr.raw-dataset-mlcommons-cognata.tags=_gdrive</code> to download the dataset from gdrive if there is any issue with rclone</li> <li>Github actions for this run can be seen here</li> </ul> <p>We have tested the workflow natively on Ubuntu 24.04, Ubuntu 22.04, Ubuntu 20.04, RHEL 9, macOS 13, macOS 14 and Windows Server 2022</p>"},{"location":"run/#results","title":"Results","text":"<p>Once the benchmark successfully has run, KPIs should print in the terminal window and the dataset frames should have been labeled.</p>"},{"location":"run/#kpis","title":"KPIs","text":"<p>For the POC only the average latency and accuracy is measured. This will change ... </p>"},{"location":"run/#labeled-frames","title":"Labeled Frames","text":"<p>In the following directory... </p> <p>IMG Before</p> <p>IMG After (w. bounding boxes)</p>"},{"location":"run/#contact","title":"Contact","text":"<p>If you face any issues, please don't hesitate to reach out!</p> <ul> <li>Raise a GitHub Issue</li> <li>Join the MLCommons Automotive Discord Server</li> </ul>"},{"location":"run/#contributors","title":"Contributors","text":"<ul> <li>The POC reference model is trained and developed by Radoyeh Shojaei</li> <li>MLPerf Loadgen integration for the POC reference is done by Grigori Fursin and Radoyeh Shojaei</li> <li>CM workflow for the POC reference is done by Arjun Suresh and Grigori Fursin</li> </ul>"},{"location":"submission/","title":"Index","text":"<p>No submission is needed for MVP</p>"},{"location":"test-abtf-model/","title":"Automating ABTF model benchmarking using CM without Docker","text":"<p>This repository contains MLCommons CM automation recipes  to make it easier to prepare and benchmark different versions of ABTF models  (public or private) with MLPerf loadgen across different software and hardware.</p>"},{"location":"test-abtf-model/#install-mlcommons-cm-automation-framework","title":"Install MLCommons CM automation framework","text":"<p>Follow this online guide  to install CM for your OS with minimal dependencies.</p>"},{"location":"test-abtf-model/#install-virtual-environment","title":"Install virtual environment","text":"<p>We tested these automation on Ubuntu and Windows.</p> <p>We suggest you to create a virtual environment to avoid messing up your Python installation.</p> <p>All CM repositories, artifacts and cache will be resided inside this virtual environment and can be removed at any time without influencing your own environment!</p>"},{"location":"test-abtf-model/#linux","title":"Linux","text":"<pre><code>python3 -m venv ABTF\nmkdir ABTF/work\n. ABTF/bin/activate ; export CM_REPOS=$PWD/ABTF/CM ; cd ABTF/work\n</code></pre>"},{"location":"test-abtf-model/#windows","title":"Windows","text":"<pre><code>python -m venv ABTF\nmd ABTF\\work\ncall ABTF\\Scripts\\activate.bat &amp; set CM_REPOS=%CD%\\ABTF\\CM &amp; cd ABTF\\work\n</code></pre>"},{"location":"test-abtf-model/#install-required-cm-automation-recipes","title":"Install required CM automation recipes","text":""},{"location":"test-abtf-model/#cm-repositories-with-automations","title":"CM repositories with automations","text":"<pre><code>cm pull repo mlcommons@cm4mlops --checkout=dev\ncm pull repo mlcommons@cm4abtf --checkout=dev\n</code></pre>"},{"location":"test-abtf-model/#show-installed-cm-repositories","title":"Show installed CM repositories","text":"<pre><code>cm show repo\n</code></pre>"},{"location":"test-abtf-model/#find-a-specific-repo","title":"Find a specific repo","text":"<pre><code>cm find repo mlcommons@cm4abtf\n</code></pre>"},{"location":"test-abtf-model/#update-all-repositories-at-any-time","title":"Update all repositories at any time","text":"<pre><code>cm pull repo\n</code></pre>"},{"location":"test-abtf-model/#turn-off-debug-info-when-running-cm-scripts","title":"Turn off debug info when running CM scripts","text":"<pre><code>cm set cfg --key.script.silent\n</code></pre>"},{"location":"test-abtf-model/#download-abtf-models-and-sample-image","title":"Download ABTF models and sample image","text":"<pre><code>cmr \"download file _wget\" --url=\"https://www.dropbox.com/scl/fi/cwyi6ukwih5qjblgh2m7x/baseline_8MP_ss_scales_fm1_5x5_all_ep60.pth?rlkey=okaq1s32leqnbjzloru206t50&amp;st=70yoyy89&amp;dl=0\" --verify_ssl=no --md5sum=26845c3b9573ce115ef29dca4ae5be14\ncmr \"download file _wget\" --url=\"https://www.dropbox.com/scl/fi/ljdnodr4buiqqwo4rgetu/baseline_4MP_ss_all_ep60.pth?rlkey=zukpgfjsxcjvf4obl64e72rf3&amp;st=umfnx8go&amp;dl=0\" --verify_ssl=no --md5sum=75e56779443f07c25501b8e43b1b094f\ncmr \"download file _wget\" --url=\"https://www.dropbox.com/scl/fi/9un2i2169rgebui4xklnm/baseline_8MP_ss_scales_all_ep60.pth?rlkey=sez3dnjep4waa09s5uy4r3wmk&amp;st=z859czgk&amp;dl=0\" --verify_ssl=no --md5sum=1ab66f523715f9564603626e94e59c8c\ncmr \"download file _wget\" --url=\"https://www.dropbox.com/scl/fi/0n7rmxxwqvg04sxk7bbum/0000008766.png?rlkey=mhmr3ztrlsqk8oa67qtxoowuh&amp;dl=0\" --verify_ssl=no --md5sum=903306a7c8bfbe6c1ca68fad6e34fe52 -s\n</code></pre>"},{"location":"test-abtf-model/#download-abtf-model-code-and-register-in-cm-cache","title":"Download ABTF model code and register in CM cache","text":"<pre><code>cmr \"get ml-model abtf-ssd-pytorch\"\n</code></pre> <p>or specific branch:</p> <pre><code>cmr \"get ml-model abtf-ssd-pytorch\" --model_code_git_branch=cognata\n</code></pre>"},{"location":"test-abtf-model/#check-the-state-of-the-cm-cache","title":"Check the state of the CM cache","text":"<pre><code>cm show cache\n</code></pre>"},{"location":"test-abtf-model/#find-model-code","title":"Find model code","text":"<pre><code>cm show cache --tags=ml-model,abtf\n</code></pre>"},{"location":"test-abtf-model/#prepare-workflow-to-benchmark-abtf-model-on-host-cpu","title":"Prepare workflow to benchmark ABTF model on host CPU","text":"<p>Next commands prepare environment to benchmark host CPU. Check these docs to benchmark other devices: * CUDA-based device</p>"},{"location":"test-abtf-model/#detect-python-from-virtual-env","title":"Detect python from virtual env","text":"<pre><code>cmr \"get python3\" --quiet\n</code></pre>"},{"location":"test-abtf-model/#install-or-detect-pytorch","title":"Install or detect PyTorch","text":"<pre><code>cmr \"get generic-python-lib _torch\"\ncmr \"get generic-python-lib _torchvision\"\n</code></pre> <p>If you want to install a specific version of PyTorch, you can specify it as follows:</p> <pre><code>cmr \"get generic-python-lib _torch\" --version=2.2.0\ncmr \"get generic-python-lib _torchvision\" --version=0.17.0\n</code></pre>"},{"location":"test-abtf-model/#test-abtf-model-with-a-sample-image-and-prepare-for-loadgen","title":"Test ABTF Model with a sample image and prepare for loadgen","text":"<pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch inference\" --model=baseline_8MP_ss_scales_all_ep60.pth --config=baseline_8MP_ss_scales_all --input=0000008766.png --output=0000008766_prediction_test_8MP.jpg --num-classes=15\ncmr \"demo abtf ssd-resnet50 cognata pytorch inference\" --model=baseline_4MP_ss_all_ep60.pth --config=baseline_4MP_ss_all --input=0000008766.png --output=0000008766_prediction_test_4MP.jpg --num-classes=15\n</code></pre> <p>CM will load a workflow described by this simple YAML, call other CM scripts to detect, download or build missing deps for a given platform, prepare all environment variables and run benchmark using the following Python code.</p> <p>You can run it in silent mode to skip CM workflow information using <code>-s</code> or <code>--silent</code> flag: <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch inference\" --model=baseline_4MP_ss_all_ep60.pth --config=baseline_4MP_ss_all --input=0000008766.png --output=0000008766_prediction_test_4MP.jpg --num-classes=15 -s\n</code></pre></p> <p>You can dump internal CM state with resolved dependencies, their versions and reproducibility README by adding flag <code>--repro</code>. You will find dump in the <code>cm-repro</code> directory of your current directory after script execution:</p> <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch inference\" --model=baseline_4MP_ss_all_ep60.pth --config=baseline_4MP_ss_all --input=0000008766.png --output=0000008766_prediction_test_4MP.jpg --num-classes=15 -s --repro\n</code></pre>"},{"location":"test-abtf-model/#test-abtf-model-inference-with-loadgen","title":"Test ABTF model inference with loadgen","text":""},{"location":"test-abtf-model/#build-mlperf-loadgen","title":"Build MLPerf loadgen","text":"<pre><code>cmr \"get mlperf inference loadgen _copy\" --version=main\n</code></pre>"},{"location":"test-abtf-model/#run-abtf-pytorch-model-with-loadgen","title":"Run ABTF PyTorch model with loadgen","text":"<p>This example uses a universal python loadgen harness with PyTorch and ONNX backends with 1 real input saved as pickle:</p> <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch inference\" --model=baseline_8MP_ss_scales_all_ep60.pth --config=baseline_8MP_ss_scales_all --input=0000008766.png --output=0000008766_prediction_test_8MP.jpg --num-classes=15\ncmr \"generic loadgen python _pytorch _custom _cmc\" --samples=5 --modelsamplepath=0000008766.png.cpu.pickle --modelpath=baseline_8MP_ss_scales_all_ep60.pth --modelcfg.num_classes=15 --modelcfg.config=baseline_8MP_ss_scales_all\n</code></pre> <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch inference\" --model=baseline_4MP_ss_all_ep60.pth --config=baseline_4MP_ss_all --input=0000008766.png --output=0000008766_prediction_test_4MP.jpg --num-classes=15\ncmr \"generic loadgen python _pytorch _custom _cmc\" --samples=5 --modelsamplepath=0000008766.png.cpu.pickle --modelpath=baseline_4MP_ss_all_ep60.pth --modelcfg.num_classes=15 --modelcfg.config=baseline_4MP_ss_all\n</code></pre>"},{"location":"test-abtf-model/#export-pytorch-abtf-model-to-onnx","title":"Export PyTorch ABTF model to ONNX","text":"<pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch inference\" --model=baseline_8MP_ss_scales_all_ep60.pth --config=baseline_8MP_ss_scales_all --input=0000008766.png --output=0000008766_prediction_test.jpg -s --export_model_to_onnx=baseline_8MP_ss_scales_all_ep60_opset17.onnx --export_model_to_onnx_opset=17\ncmr \"demo abtf ssd-resnet50 cognata pytorch inference\" --model=baseline_4MP_ss_all_ep60.pth --config=baseline_4MP_ss_all --input=0000008766.png --output=0000008766_prediction_test.jpg -s --export_model_to_onnx=baseline_4MP_ss_all_ep60_opset17.onnx --export_model_to_onnx_opset=17\n</code></pre>"},{"location":"test-abtf-model/#run-abtf-onnx-model-with-loadgen-and-random-input","title":"Run ABTF ONNX model with loadgen and random input","text":"<pre><code>cm run script \"generic loadgen python _onnxruntime\" --samples=5 --modelpath=baseline_8MP_ss_scales_all_ep60_opset17.onnx --output_dir=results --repro -s\ncm run script \"generic loadgen python _onnxruntime\" --samples=5 --modelpath=baseline_4MP_ss_all_ep60_opset17.onnx --output_dir=results --repro -s\n</code></pre>"},{"location":"test-abtf-model/#try-quantization-via-huggingface-quanto-package","title":"Try quantization via HuggingFace quanto package","text":"<p>We added simple example to do basic and automatic quantization of the model to int8 using HugginFace's quanto package:</p> <pre><code>cm run script \"demo abtf ssd-resnet50 cognata pytorch inference\" \n     --ad.ml-model.model_code_git_branch=cognata \\\n     --model=baseline_8MP_ss_scales_all_ep60_state.pth \\\n     --config=baseline_8MP_ss_scales_all \\\n     --input=0000008766.png \\\n     --output=0000008766_prediction_test_8MP_quantized.jpg \\\n     --quantize_with_huggingface_quanto \\\n     --repro\n</code></pre>"},{"location":"test-abtf-model/#run-inference-with-quantized-model","title":"Run inference with quantized model","text":"<pre><code>cm run script \"demo abtf ssd-resnet50 cognata pytorch inference\" \\\n     --ad.ml-model.model_code_git_branch=cognata \\\n     --model=baseline_8MP_ss_scales_all_ep60_state_hf_quanto_qint8.pth \\\n     --config=baseline_8MP_ss_scales_all \\\n     --model_quanto \\\n     --input=0000008766.png \\\n     --output=0000008766_prediction_test_8MP_quantized_inference.jpg \\\n     --repro -s\n</code></pre>"},{"location":"test-abtf-model/#archive-of-exported-models","title":"Archive of exported models","text":"<p>You can find all exported models in this DropBox folder.</p> <p>You can download individual files via CM as follows: <pre><code>cmr \"download file _wget\" --url=\"https://www.dropbox.com/scl/fi/yj3m4vpudmlqgkdkaatk0/baseline_4MP_ss_all_ep60_opset17.onnx?rlkey=r9i3ew2hfajzyssvmwb0vytbg&amp;st=vkb3fwc7&amp;dl=0\" --verify_ssl=no --md5sum=7fbd70f7e1a0c0fbc1464f85351bce4b\n</code></pre></p>"},{"location":"test-abtf-model/#test-abtf-model-with-a-cognata-sub-set","title":"Test ABTF model with a Cognata sub-set","text":"<p>We have developed CM script to automate management and download of the MLCommons Cognata dataset.  You just need to register here and obtain a private URL that you will need to enter once:</p> <pre><code>cmr \"get raw dataset mlcommons-cognata\" --serial_numbers=10002_Urban_Clear_Morning --group_names=Cognata_Camera_01_8M --file_names=\"Cognata_Camera_01_8M_ann.zip;Cognata_Camera_01_8M_ann_laneline.zip;Cognata_Camera_01_8M.zip\"\n\ncmr \"demo abtf ssd-resnet50 cognata pytorch inference _dataset\" --model=baseline_4MP_ss_all_ep60.pth --config=baseline_4MP_ss_all\n\ncmr \"demo abtf ssd-resnet50 cognata pytorch inference _dataset\" --model=baseline_4MP_ss_all_ep60.pth --config=baseline_4MP_ss_all --visualize\n</code></pre>"},{"location":"test-abtf-model/#benchmark-accuracy-of-abtf-model-with-mlperf-loadgen","title":"Benchmark accuracy of ABTF model with MLPerf loadgen","text":""},{"location":"test-abtf-model/#download-cognata-data-set-via-cm","title":"Download Cognata data set via CM","text":"<p>We have developed a CM automation recipe  to download different sub-sets of the MLCommons Cognata data set - you just need to provide a private URL after registering to access  this dataset here:</p> <p><pre><code>cmr \"get raw dataset mlcommons-cognata\" --serial_numbers=10002_Urban_Clear_Morning --group_names=Cognata_Camera_01_8M --file_names=\"Cognata_Camera_01_8M_ann.zip;Cognata_Camera_01_8M_ann_laneline.zip;Cognata_Camera_01_8M.zip\"\n</code></pre> or in a more compact way via variation</p> <pre><code>cmr \"get raw dataset mlcommons-cognata _abtf-demo\"\n</code></pre> <p>and to some specific path <pre><code>cmr \"get raw dataset mlcommons-cognata _abtf-demo\" --path=/cognata\n</code></pre></p> <p>or import the existing one <pre><code>cmr \"get raw dataset mlcommons-cognata _abtf-demo\" --import=/cognata\n</code></pre></p>"},{"location":"test-abtf-model/#run-basic-mlperf-harness-for-abtf-model","title":"Run basic MLPerf harness for ABTF model","text":"<p>We've prototyped a basic MLPerf harness with CM automation to benchmark ABTF model with Cognata data set.  See CM script  and MLPerf Python harness for Cognata and ABTF model. It should work on Linux and Windows with CPU and CUDA GPU.</p> <p>You can run performance test with the ABTF model and Cognata datset on CPU as follows (use --count to select the number of samples):</p> <pre><code>cmr \"download file _wget\" --url=\"https://www.dropbox.com/scl/fi/9un2i2169rgebui4xklnm/baseline_8MP_ss_scales_all_ep60.pth?rlkey=sez3dnjep4waa09s5uy4r3wmk&amp;st=z859czgk&amp;dl=0\" --verify_ssl=no --md5sum=1ab66f523715f9564603626e94e59c8c\n\ncmr \"get raw dataset mlcommons-cognata _abtf-demo\" --import=/cognata\n\ncm run script --tags=run-mlperf-inference,demo,abtf-model,_abtf-demo-model,_pytorch \\\n   --device=cpu \\\n   --dataset=cognata-8mp-pt \\\n   --model=$PWD/baseline_8MP_ss_scales_all_ep60.pth \\\n   --env.CM_ABTF_ML_MODEL_CONFIG=baseline_8MP_ss_scales_all \\\n   --env.CM_ABTF_NUM_CLASSES=15 \\\n   --env.CM_DATASET_MLCOMMONS_COGNATA_SERIAL_NUMBERS=10002_Urban_Clear_Morning \\\n   --env.CM_DATASET_MLCOMMONS_COGNATA_GROUP_NAMES=Cognata_Camera_01_8M \\\n   --env.CM_ABTF_ML_MODEL_TRAINING_FORCE_COGNATA_LABELS=yes \\\n   --env.CM_ABTF_ML_MODEL_SKIP_WARMUP=yes \\\n   --max_batchsize=1 \\\n   --count=2 \\\n   --precision=float32 \\\n   --implementation=mlcommons-python \\\n   --scenario=Offline \\\n   --mode=performance \\\n   --power=no \\\n   --adr.python.version_min=3.8 \\\n   --adr.compiler.tags=gcc \\\n   --output=$PWD/results \\\n   --rerun \\\n   --clean\n</code></pre> <p>You should see the following output: <pre><code>======================================================================\n\nLoading model ...\n\n\nLoading dataset and preprocessing if needed ...\n* Dataset path: /cognata\n* Preprocessed cache path: /home/cmuser/CM/repos/mlcommons@cm4abtf/script/demo-ml-model-abtf-cognata-pytorch-loadgen/tmp-preprocessed-dataset\n\n\nCognata folders: ['10002_Urban_Clear_Morning']\nCognata cameras: ['Cognata_Camera_01_8M']\n\n\nScanning Cognata dataset ...\n  Number of files found: 1020\n  Time: 2.36 sec.\n\nPreloading and preprocessing Cognata dataset on the fly ...\n  Time: 0.62 sec.\n\nTestScenario.Offline qps=3.73, mean=0.5024, time=0.537, queries=2, tiles=50.0:0.5024,80.0:0.5056,90.0:0.5067,95.0:0.5072,99.0:0.5076,99.9:0.5077\n</code></pre></p> <p>You can run MLPerf accuracy test by changing <code>--mode=performance</code> to <code>--mode=accuracy</code>  in the above command line and rerun it. You should the output similar to the following:</p> <pre><code>======================================================================\n\nLoading model ...\n\n\nLoading dataset and preprocessing if needed ...\n* Dataset path: /cognata\n* Preprocessed cache path: /home/cmuser/CM/repos/mlcommons@cm4abtf/script/demo-ml-model-abtf-cognata-pytorch-loadgen/tmp-preprocessed-dataset\n\n\nCognata folders: ['10002_Urban_Clear_Morning']\nCognata cameras: ['Cognata_Camera_01_8M']\n\n\nScanning Cognata dataset ...\n  Number of files found: 1020\n  Time: 2.36 sec.\n\nPreloading and preprocessing Cognata dataset on the fly ...\n  Time: 0.64 sec.\n\n=================================================\n{   'classes': tensor([ 2,  3,  4,  5,  9, 10], dtype=torch.int32),\n    'map': tensor(0.3814),\n    'map_50': tensor(0.6143),\n    'map_75': tensor(0.4557),\n    'map_large': tensor(0.5470),\n    'map_medium': tensor(0.4010),\n    'map_per_class': tensor([0.0000, 0.1002, 0.6347, 0.7505, 0.1525, 0.6505]),\n    'map_small': tensor(0.0037),\n    'mar_1': tensor(0.2909),\n    'mar_10': tensor(0.3833),\n    'mar_100': tensor(0.3833),\n    'mar_100_per_class': tensor([0.0000, 0.1000, 0.6500, 0.7500, 0.1500, 0.6500]),\n    'mar_large': tensor(0.5500),\n    'mar_medium': tensor(0.4000),\n    'mar_small': tensor(0.0071)}\n=================================================\nTestScenario.Offline qps=1.81, mean=0.7691, time=1.107, acc=0.000%, mAP=38.140%, mAP_classes={'Props': 0.0, 'TrafficSign': 0.10024752467870712, 'Car': 0.6346888542175293, 'Van': 0.7504950761795044, 'Pedestrian': 0.1524752527475357, 'Truck': 0.6504950523376465}, queries=2, tiles=50.0:0.7691,80.0:0.7713,90.0:0.7721,95.0:0.7725,99.0:0.7728,99.9:0.7728\n======================================================================\n</code></pre> <p>Please check CM CUDA docs to benchmark ABTF model using MLPerf loadgen.</p>"},{"location":"test-abtf-model/#using-docker","title":"Using Docker","text":"<p>We prepared a demo to use Docker with CM automation for ABTF. You can find and run scripts to build and run Docker (CPU &amp; CUDA) with CM  here.</p> <p>You can then use above commands to benchmark ABTF model. Don't forget to set up your private GitHub tocken to be able  to pull a [private GitHub repo with ABTF model](https://github.com/mlcommons/abtf-ssd-pytorch/tree/cognata</p> <pre><code>export CM_GH_TOKEN=\"YOUR TOKEN\"\n</code></pre>"},{"location":"test-abtf-model/#further-developments","title":"Further developments","text":"<p>Current CM for ABTF roadmap here. MLCommons will fund a CM engineer to automate MLPerf inference until the end of 2024 -  further developments will be done in sync with the MLCommons Inference Working Group.</p>"},{"location":"test-abtf-model/#feedback","title":"Feedback","text":"<p>Join MLCommons discord server with ABTF channel</p>"},{"location":"test-abtf-model/#acknowledgments","title":"Acknowledgments","text":"<p>CM automation and MLPerf harness prototyping was sponsored by cKnowledge.org based on feedback from ABTF members.</p>"},{"location":"test-abtf-model/README-cuda/","title":"README cuda","text":"<p>[ Back to the main page ]</p>"},{"location":"test-abtf-model/README-cuda/#benchmark-abtf-model-on-cuda-based-device-using-cm-without-docker","title":"Benchmark ABTF model on CUDA-based device using CM without Docker","text":""},{"location":"test-abtf-model/README-cuda/#prerequisites","title":"Prerequisites","text":"<ul> <li>We expect that you already have CUDA driver installed</li> <li>Tested with </li> <li>Python 3.11.x (3.12+ currently doesn't work)</li> <li>CUDA 11.8 and 12.1 (with cuDNN)</li> <li>torch 2.2.2 and 2.3.0</li> <li>torchvision 0.17.1 and 0.18.0 </li> <li>Didn't work on Windows with</li> <li>Python 3.12.x</li> <li>CUDA 12.1 with cuDNN</li> <li>torch 2.3.0</li> <li>torchvision 0.18.0 </li> </ul>"},{"location":"test-abtf-model/README-cuda/#detect-or-install-cuda-toolkit-and-libraries","title":"Detect or install CUDA toolkit and libraries","text":"<pre><code>cmr \"get cuda _toolkit\"\ncmr \"get cuda-devices\"\n</code></pre> <p>If you need to use cuDNN, use the following command:</p> <pre><code>cmr \"get cuda _toolkit _cudnn\"\n</code></pre> <p>If cuDNN is not installed, you can download it from the website and register via CM as follows: <pre><code>cmr \"get cudnn\" --tar_file={FULL PATH TO cudnn TAR file}\n</code></pre></p>"},{"location":"test-abtf-model/README-cuda/#build-mlperf-loadgen","title":"Build MLPerf loadgen","text":"<pre><code>cmr \"get mlperf inference loadgen _copy\" --version=main\n</code></pre>"},{"location":"test-abtf-model/README-cuda/#install-or-detect-pytorch-and-pytorchvision","title":"Install or detect PyTorch and PyTorchVision","text":"<p>TBD: better automation of CUDA version detection and passing extra-index-url</p>"},{"location":"test-abtf-model/README-cuda/#cuda-118","title":"CUDA 11.8","text":"<pre><code>cmr \"get generic-python-lib _torch_cuda\" --extra-index-url=https://download.pytorch.org/whl/cu118 --force-install\ncmr \"get generic-python-lib _torchvision_cuda\" --extra-index-url=https://download.pytorch.org/whl/cu118 --force-install\n</code></pre>"},{"location":"test-abtf-model/README-cuda/#cuda-121","title":"CUDA 12.1","text":"<pre><code>cmr \"get generic-python-lib _torch_cuda\" --extra-index-url=https://download.pytorch.org/whl/cu121 --force-install --version=2.2.2\ncmr \"get generic-python-lib _torchvision_cuda\" --extra-index-url=https://download.pytorch.org/whl/cu121 --force-install --version=0.17.1\n</code></pre>"},{"location":"test-abtf-model/README-cuda/#cuda-122","title":"CUDA 12.2","text":"<pre><code>cmr \"get generic-python-lib _torch_cuda\" --extra-index-url=https://download.pytorch.org/whl/cu122 --force-install --version=2.2.2\ncmr \"get generic-python-lib _torchvision_cuda\" --extra-index-url=https://download.pytorch.org/whl/cu122 --force-install --version=0.17.1\n</code></pre>"},{"location":"test-abtf-model/README-cuda/#test-model-with-a-test-image","title":"Test Model with a test image","text":"<pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch inference _cuda\" \\\n     --model=baseline_4MP_ss_all_ep60.pth \\\n     --config=baseline_4MP_ss_all \\\n     --input=0000008766.png \n     --output=0000008766_prediction_test.jpg \\\n     --repro -s\n</code></pre>"},{"location":"test-abtf-model/README-cuda/#check-with-cognata-demo","title":"Check with Cognata demo","text":"<pre><code>cmr \"get raw dataset mlcommons-cognata\" --serial_numbers=10002_Urban_Clear_Morning --group_names=Cognata_Camera_01_8M --file_names=\"Cognata_Camera_01_8M_ann.zip;Cognata_Camera_01_8M_ann_laneline.zip;Cognata_Camera_01_8M.zip\"\ncmr \"demo abtf ssd-resnet50 cognata pytorch inference _cuda _dataset\" --model=baseline_4MP_ss_all_ep60.pth --config=baseline_4MP_ss_all --visualize\n</code></pre>"},{"location":"test-abtf-model/README-cuda/#benchmark-model-with-mlperf-loadgen-just-1-performance-sample","title":"Benchmark model with MLPerf loadgen (just 1 performance sample)","text":"<pre><code>cm run script \"generic loadgen python _pytorch _cuda _custom _cmc\" \\\n     --samples=5 \\\n     --modelsamplepath=0000008766.png.cuda.pickle \\\n     --modelpath=baseline_4MP_ss_all_ep60.pth \\\n     --modelcfg.num_classes=15 \\\n     --modelcfg.config=baseline_4MP_ss_all \\\n     --output_dir=results \\\n     --repro -s\n</code></pre>"},{"location":"test-abtf-model/README-cuda/#benchmark-accuracy-of-abtf-model-with-mlperf-loadgen","title":"Benchmark accuracy of ABTF model with MLPerf loadgen","text":""},{"location":"test-abtf-model/README-cuda/#download-cognata-data-set-via-cm","title":"Download Cognata data set via CM","text":"<p>We have developed a CM automation recipe  to download different sub-sets of the MLCommons Cognata data set - you just need to provide a private URL after registering to access  this dataset here:</p> <p><pre><code>cmr \"get raw dataset mlcommons-cognata\" --serial_numbers=10002_Urban_Clear_Morning --group_names=Cognata_Camera_01_8M --file_names=\"Cognata_Camera_01_8M_ann.zip;Cognata_Camera_01_8M_ann_laneline.zip;Cognata_Camera_01_8M.zip\"\n</code></pre> or in a more compact way via variation</p> <pre><code>cmr \"get raw dataset mlcommons-cognata _abtf-demo\"\n</code></pre> <p>and to some specific path <pre><code>cmr \"get raw dataset mlcommons-cognata _abtf-demo\" --path=/cognata\n</code></pre></p> <p>or import the existing one <pre><code>cmr \"get raw dataset mlcommons-cognata _abtf-demo\" --import=/cognata\n</code></pre></p>"},{"location":"test-abtf-model/README-cuda/#run-basic-mlperf-harness-for-abtf-model","title":"Run basic MLPerf harness for ABTF model","text":"<p>We've prototyped a basic MLPerf harness with CM automation to benchmark ABTF model with Cognata data set.  See CM script  and MLPerf Python harness for Cognata and ABTF model. It should work on Linux and Windows with CPU and CUDA GPU.</p> <p>You can run performance test with the ABTF model and Cognata datset on CPU as follows (use --count to select the number of samples):</p> <pre><code>cmr \"download file _wget\" --url=\"https://www.dropbox.com/scl/fi/9un2i2169rgebui4xklnm/baseline_8MP_ss_scales_all_ep60.pth?rlkey=sez3dnjep4waa09s5uy4r3wmk&amp;st=z859czgk&amp;dl=0\" --verify_ssl=no --md5sum=1ab66f523715f9564603626e94e59c8c\n\ncmr \"get raw dataset mlcommons-cognata _abtf-demo\" --import=/cognata\n\ncm run script --tags=run-mlperf-inference,demo,abtf-model,_abtf-demo-model,_pytorch,_cuda \\\n   --dataset=cognata-8mp-pt \\\n   --model=$PWD/baseline_8MP_ss_scales_all_ep60.pth \\\n   --env.CM_ABTF_ML_MODEL_CONFIG=baseline_8MP_ss_scales_all \\\n   --env.CM_ABTF_NUM_CLASSES=15 \\\n   --env.CM_DATASET_MLCOMMONS_COGNATA_SERIAL_NUMBERS=10002_Urban_Clear_Morning \\\n   --env.CM_DATASET_MLCOMMONS_COGNATA_GROUP_NAMES=Cognata_Camera_01_8M \\\n   --env.CM_ABTF_ML_MODEL_TRAINING_FORCE_COGNATA_LABELS=yes \\\n   --env.CM_ABTF_ML_MODEL_SKIP_WARMUP=yes \\\n   --max_batchsize=1 \\\n   --count=2 \\\n   --precision=float32 \\\n   --implementation=mlcommons-python \\\n   --scenario=Offline \\\n   --mode=performance \\\n   --power=no \\\n   --adr.python.version_min=3.8 \\\n   --adr.compiler.tags=gcc \\\n   --output=$PWD/results \\\n   --rerun \\\n   --clean\n</code></pre> <p>You should see the following output: <pre><code>======================================================================\n\nLoading model ...\n\n\nLoading dataset and preprocessing if needed ...\n* Dataset path: /cognata\n* Preprocessed cache path: /home/cmuser/CM/repos/mlcommons@cm4abtf/script/demo-ml-model-abtf-cognata-pytorch-loadgen/tmp-preprocessed-dataset\n\n\nCognata folders: ['10002_Urban_Clear_Morning']\nCognata cameras: ['Cognata_Camera_01_8M']\n\n\nScanning Cognata dataset ...\n  Number of files found: 1020\n  Time: 2.36 sec.\n\nPreloading and preprocessing Cognata dataset on the fly ...\n  Time: 0.62 sec.\n\nTestScenario.Offline qps=3.73, mean=0.5024, time=0.537, queries=2, tiles=50.0:0.5024,80.0:0.5056,90.0:0.5067,95.0:0.5072,99.0:0.5076,99.9:0.5077\n</code></pre></p> <p>You can run MLPerf accuracy test by changing <code>--mode=performance</code> to <code>--mode=accuracy</code>  in the above command line and rerun it. You should the output similar to the following:</p> <pre><code>======================================================================\n\nLoading model ...\n\n\nLoading dataset and preprocessing if needed ...\n* Dataset path: /cognata\n* Preprocessed cache path: /home/cmuser/CM/repos/mlcommons@cm4abtf/script/demo-ml-model-abtf-cognata-pytorch-loadgen/tmp-preprocessed-dataset\n\n\nCognata folders: ['10002_Urban_Clear_Morning']\nCognata cameras: ['Cognata_Camera_01_8M']\n\n\nScanning Cognata dataset ...\n  Number of files found: 1020\n  Time: 2.36 sec.\n\nPreloading and preprocessing Cognata dataset on the fly ...\n  Time: 0.64 sec.\n\n=================================================\n{   'classes': tensor([ 2,  3,  4,  5,  9, 10], dtype=torch.int32),\n    'map': tensor(0.3814),\n    'map_50': tensor(0.6143),\n    'map_75': tensor(0.4557),\n    'map_large': tensor(0.5470),\n    'map_medium': tensor(0.4010),\n    'map_per_class': tensor([0.0000, 0.1002, 0.6347, 0.7505, 0.1525, 0.6505]),\n    'map_small': tensor(0.0037),\n    'mar_1': tensor(0.2909),\n    'mar_10': tensor(0.3833),\n    'mar_100': tensor(0.3833),\n    'mar_100_per_class': tensor([0.0000, 0.1000, 0.6500, 0.7500, 0.1500, 0.6500]),\n    'mar_large': tensor(0.5500),\n    'mar_medium': tensor(0.4000),\n    'mar_small': tensor(0.0071)}\n=================================================\nTestScenario.Offline qps=1.81, mean=0.7691, time=1.107, acc=0.000%, mAP=38.140%, mAP_classes={'Props': 0.0, 'TrafficSign': 0.10024752467870712, 'Car': 0.6346888542175293, 'Van': 0.7504950761795044, 'Pedestrian': 0.1524752527475357, 'Truck': 0.6504950523376465}, queries=2, tiles=50.0:0.7691,80.0:0.7713,90.0:0.7721,95.0:0.7725,99.0:0.7728,99.9:0.7728\n======================================================================\n</code></pre> <p>Please check CM CUDA docs to benchmark ABTF model using MLPerf loadgen.</p>"},{"location":"test-abtf-model/README-cuda/#using-docker","title":"Using Docker","text":"<p>We prepared a demo to use Docker with CM automation for ABTF. You can find and run scripts to build and run Docker (CPU &amp; CUDA) with CM  here.</p> <p>You can then use above commands to benchmark ABTF model. Don't forget to set up your private GitHub tocken to be able  to pull a [private GitHub repo with ABTF model](https://github.com/mlcommons/abtf-ssd-pytorch/tree/cognata</p> <pre><code>export CM_GH_TOKEN=\"YOUR TOKEN\"\n</code></pre>"},{"location":"test-abtf-model/README-cuda/#feedback","title":"Feedback","text":"<p>Join MLCommons discord or get in touch with developers: Radoyeh Shojaei and Grigori Fursin</p>"},{"location":"test-abtf-model/README-kb/","title":"Issues, notes and prototypes","text":""},{"location":"test-abtf-model/README-kb/#grigori-weird-case-on-windows","title":"Grigori: weird case on Windows","text":"<p>If I download baseline_8mp_ss_scales_ep15.pth to the ROOT directory with the virtual environment, pip stops working since it considers this file as a broken package ...</p>"},{"location":"test-abtf-model/README-kb/#grigori-misc-commands","title":"Grigori: misc commands","text":"<p>Register local ABTF model in CM cache to be the default</p> <pre><code>cmr \"get ml-model abtf-ssd-pytorch _local.baseline_8mp_ss_scales_ep15.pth\"\n</code></pre>"},{"location":"test-abtf-model/README-kb/#grigori-benchmarking-other-models","title":"Grigori: benchmarking other models","text":"<p>Other ways to download public or private model code and weights: <pre><code>cmr \"get ml-model abtf-ssd-pytorch _skip_weights\" --adr.abtf-ml-model-code-git-repo.env.CM_ABTF_MODEL_CODE_GIT_URL=https://github.com/mlcommons/abtf-ssd-pytorch\ncmr \"get ml-model abtf-ssd-pytorch _skip_weights\" --model_code_git_url=https://github.com/mlcommons/abtf-ssd-pytorch --model_code_git_branch=cognata-cm\ncmr \"get ml-model abtf-ssd-pytorch _skip_weights _skip_code\"\n</code></pre></p> <p>Other ways to run local (private) model:</p> <p>You can first copy ABTF model code from GitHub to your local directory <code>my-model-code</code>.</p> <pre><code>cmr \"generic loadgen python _pytorch _custom _cmc\" --samples=5 --modelsamplepath=0000008766.png.cpu.pickle \\\n  --modelpath=baseline_8mp_ss_scales_ep15.pth \\\n  --modelcfg.num_classes=13 \\\n  --modelcodepath=\"my-model-code\" \\\n  --modelcfg.config=baseline_8MP_ss_scales\n</code></pre> <p>On CUDA GPU:</p> <pre><code>cmr \"generic loadgen python _pytorch _cuda _custom _cmc\" --samples=5 --modelsamplepath=0000008766.png.cpu.pickle \\\n  --modelpath=baseline_8mp_ss_scales_ep15.pth \\\n  --modelcfg.num_classes=13 \\\n  --modelcodepath=\"my-model-code\" \\\n  --modelcfg.config=baseline_8MP_ss_scales\n</code></pre>"},{"location":"test-abtf-model/README-kb/#grigori-import-cognata-dataset-from-local-folder","title":"Grigori: import Cognata dataset from local folder","text":"<pre><code>cmr \"get raw dataset mlcommons-cognata\" --import=D:\\Work2\\cognata\n</code></pre>"},{"location":"test-abtf-model/README-training/","title":"README training","text":"<p>[ Back to the main page ]</p>"},{"location":"test-abtf-model/README-training/#automating-abtf-model-training-using-cm-and-docker","title":"Automating ABTF model training using CM and Docker","text":"<p>This repository contains MLCommons CM automation recipes  to make it easier to prepare and benchmark different versions of ABTF models  (public or private) with MLPerf loadgen across different software and hardware.</p>"},{"location":"test-abtf-model/README-training/#start-pre-generated-cm-cuda-container-for-abtf","title":"Start pre-generated CM-CUDA container for ABTF","text":""},{"location":"test-abtf-model/README-training/#import-or-download-mlcommons-cognata-data-set","title":"Import or download MLCommons Cognata data set","text":"<p>Import Cognata data set downloaded manually or via CM on the host and exposed to Docker via <code>/cognata</code> path:</p> <pre><code>cmr \"get raw dataset mlcommons-cognata\" --import=/cognata\n</code></pre> <p>or download Cognata data set:</p> <pre><code>cmr \"get raw dataset mlcommons-cognata\" --path=/cognata\n</code></pre>"},{"location":"test-abtf-model/README-training/#run-abtf-model-training","title":"Run ABTF model training","text":"<p>Using full dataset:</p> <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch training _cuda\" --config=baseline_4MP_ss_all \n</code></pre> <p>or partial dataset: <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch training _cuda\" --config=baseline_4MP_ss_all --dataset_folders=10002_Urban_Clear_Morning --dataset_cameras=Cognata_Camera_01_8M\n</code></pre></p> <p>or on Windows with gloo:</p> <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch training _cuda\" --config=baseline_4MP_ss_all --torch_distributed_type=gloo --torch_distributed_init=\"\" --dataset_folders=10002_Urban_Clear_Morning --dataset_cameras=Cognata_Camera_01_8M\n</code></pre> <p>See related CM script.</p>"},{"location":"test-abtf-model/README-training/#run-abtf-model-evaluation","title":"Run ABTF model evaluation","text":"<pre><code>cmr \"download file _wget\" --url=\"https://www.dropbox.com/scl/fi/ljdnodr4buiqqwo4rgetu/baseline_4MP_ss_all_ep60.pth?rlkey=zukpgfjsxcjvf4obl64e72rf3&amp;st=umfnx8go&amp;dl=0\" --verify_ssl=no --md5sum=75e56779443f07c25501b8e43b1b094f\ncmr \"demo abtf ssd-resnet50 cognata pytorch evaluation _cuda\" --config=baseline_4MP_ss_all --pretrained_model=baseline_4MP_ss_all_ep60.pth\n</code></pre> <p>Run on partial dataset (just for a test): <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch evaluation _cuda\" --config=baseline_4MP_ss_all --dataset_folders=10002_Urban_Clear_Morning --dataset_cameras=Cognata_Camera_01_8M --pretrained_model=baseline_4MP_ss_all_ep60.pth --force_cognata_labels=yes\n</code></pre> or on Windows: <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch evaluation _cuda\" --config=baseline_4MP_ss_all --torch_distributed_type=gloo --torch_distributed_init=\"\" --dataset_folders=10002_Urban_Clear_Morning --dataset_cameras=Cognata_Camera_01_8M --pretrained_model=baseline_4MP_ss_all_ep60.pth --force_cognata_labels=yes\n</code></pre></p> <p>See related CM script.</p>"},{"location":"test-abtf-model/README-training/#run-abtf-model-test","title":"Run ABTF model test","text":"<pre><code>cmr \"download file _wget\" --url=\"https://www.dropbox.com/scl/fi/ljdnodr4buiqqwo4rgetu/baseline_4MP_ss_all_ep60.pth?rlkey=zukpgfjsxcjvf4obl64e72rf3&amp;st=umfnx8go&amp;dl=0\" --verify_ssl=no --md5sum=75e56779443f07c25501b8e43b1b094f\ncmr \"demo abtf ssd-resnet50 cognata pytorch test _cuda\" --config=baseline_4MP_ss_all --model=baseline_4MP_ss_all_ep60.pth\n</code></pre> <p>Run on partial dataset (just for a test): <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch test _cuda\" --config=baseline_4MP_ss_all --dataset_folders=10002_Urban_Clear_Morning --dataset_cameras=Cognata_Camera_01_8M --pretrained_model=baseline_4MP_ss_all_ep60.pth --force_cognata_labels=yes\n</code></pre> or on Windows: <pre><code>cmr \"demo abtf ssd-resnet50 cognata pytorch test _cuda\" --config=baseline_4MP_ss_all --dataset_folders=10002_Urban_Clear_Morning --dataset_cameras=Cognata_Camera_01_8M --pretrained_model=baseline_4MP_ss_all_ep60.pth --force_cognata_labels=yes\n</code></pre></p> <p>See related CM script.</p>"},{"location":"test-abtf-model/docker/cognata/","title":"Index","text":"<p>Placeholder for MLCommons Cognata data set.</p>"},{"location":"test-abtf-model/docker/trained_models/","title":"Index","text":"<p>Placeholder for ABTF models.</p>"}]}